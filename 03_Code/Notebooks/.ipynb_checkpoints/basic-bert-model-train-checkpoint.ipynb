{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-03-12T12:06:52.023480Z",
     "iopub.status.busy": "2022-03-12T12:06:52.023193Z",
     "iopub.status.idle": "2022-03-12T12:06:52.060071Z",
     "shell.execute_reply": "2022-03-12T12:06:52.059352Z",
     "shell.execute_reply.started": "2022-03-12T12:06:52.023402Z"
    }
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-12T12:06:52.062208Z",
     "iopub.status.busy": "2022-03-12T12:06:52.061747Z",
     "iopub.status.idle": "2022-03-12T12:07:25.345902Z",
     "shell.execute_reply": "2022-03-12T12:07:25.345080Z",
     "shell.execute_reply.started": "2022-03-12T12:06:52.062171Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install Arabic-Stopwords\n",
    "!pip install arabic-reshaper\n",
    "!pip install python-bidi\n",
    "!pip install livelossplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-12T12:07:25.347927Z",
     "iopub.status.busy": "2022-03-12T12:07:25.347657Z",
     "iopub.status.idle": "2022-03-12T12:07:25.354262Z",
     "shell.execute_reply": "2022-03-12T12:07:25.353606Z",
     "shell.execute_reply.started": "2022-03-12T12:07:25.347893Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def read_csv(file_path):\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "\n",
    "def read_txt(file_path):\n",
    "    return set(open(file_path, encoding='utf-8').readlines())\n",
    "\n",
    "\n",
    "def read_arabic_csv(file_path):\n",
    "    df = pd.read_csv(file_path, lineterminator='\\n', encoding='utf-8')\n",
    "    df.columns = [col.replace('\\r', '') for col in df.columns]\n",
    "    return df.replace({r'\\r': ''}, regex=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-12T12:07:25.357054Z",
     "iopub.status.busy": "2022-03-12T12:07:25.356748Z",
     "iopub.status.idle": "2022-03-12T12:07:25.371318Z",
     "shell.execute_reply": "2022-03-12T12:07:25.370547Z",
     "shell.execute_reply.started": "2022-03-12T12:07:25.357018Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from requests.packages import urllib3\n",
    "import json\n",
    "from os.path import join\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "\n",
    "url = 'https://recruitment.aimtechnologies.co/ai-tasks'\n",
    "\n",
    "\n",
    "def get_tweets_api(ids_list):\n",
    "    return requests.post(url,\n",
    "                         headers={'Content-Type': 'application/json'},\n",
    "                         data=json.dumps(ids_list),\n",
    "                         verify=False).json()\n",
    "\n",
    "\n",
    "def get_dataset_df(df, save_directory_path=\"Dataset\"):\n",
    "    len_df = len(df)\n",
    "    count = 0\n",
    "    id_list = []\n",
    "    dialect_list = []\n",
    "    text_list = []\n",
    "    while len_df > 0:\n",
    "        num_samples = min(1000, len_df)\n",
    "        end_index = count+num_samples-1\n",
    "        ids_list = list(map(str, df.loc[count: end_index, \"id\"].values))\n",
    "        json_dataset = get_tweets_api(ids_list)\n",
    "        id_list.extend(ids_list)\n",
    "        dialect_list.extend(list(df.loc[count: end_index, \"dialect\"].values))\n",
    "        text_list.extend(json_dataset.values())\n",
    "        count = end_index\n",
    "        len_df -= num_samples\n",
    "\n",
    "    res_df = pd.DataFrame(list(zip(id_list, dialect_list, text_list)),\n",
    "                          columns=['Id', 'Dialect', \"Text\"])\n",
    "    csv_file = join(save_directory_path, \"csv_text_dataset.csv\")\n",
    "    res_df.to_csv(csv_file, index=False, encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-12T12:07:25.373632Z",
     "iopub.status.busy": "2022-03-12T12:07:25.373012Z",
     "iopub.status.idle": "2022-03-12T12:07:26.853323Z",
     "shell.execute_reply": "2022-03-12T12:07:26.851834Z",
     "shell.execute_reply.started": "2022-03-12T12:07:25.373578Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from string import punctuation\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "import arabicstopwords.arabicstopwords as stp\n",
    "punctuation += '،؛؟”“'\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "def remove_emoji(text):\n",
    "    regex_pattern = re.compile(\"[\"\n",
    "                            u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                            u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                            u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                            u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                            u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "                            u\"\\U00002702-\\U000027B0\"\n",
    "                            u\"\\U00002702-\\U000027B0\"\n",
    "                            u\"\\U000024C2-\\U0001F251\"\n",
    "                            u\"\\U0001f926-\\U0001f937\"\n",
    "                            u\"\\U00010000-\\U0010ffff\"\n",
    "                            u\"\\u2640-\\u2642\" \n",
    "                            u\"\\u2600-\\u2B55\"\n",
    "                            u\"\\u200d\"\n",
    "                            u\"\\u23cf\"\n",
    "                            u\"\\u23e9\"\n",
    "                            u\"\\u231a\"\n",
    "                            u\"\\ufe0f\"  # dingbats\n",
    "                            u\"\\u3030\"\n",
    "                            \"]+\", flags=re.UNICODE)\n",
    "\n",
    "    return regex_pattern.sub(r'', text)\n",
    "\n",
    "\n",
    "def remove_email(text):\n",
    "    return re.sub('([A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,})', '', text)\n",
    "\n",
    "\n",
    "def remove_repeated_char(text):\n",
    "    return re.sub(r'(.)\\1\\1{1,}', r'\\1\\1', text)\n",
    "\n",
    "\n",
    "def remove_account_tag(text):\n",
    "    return re.sub(r'@[\\w]+', '', text)\n",
    "\n",
    "\n",
    "def remove_hashtag(text):\n",
    "    return re.sub(r'#[\\w]+', '', text)\n",
    "\n",
    "\n",
    "def remove_links(text):\n",
    "    return re.sub(r'http[^\\s]+', '', text)\n",
    "\n",
    "\n",
    "def remove_spaces(text):\n",
    "    text = re.sub(r\"\\n+\", ' ', text)\n",
    "    text = re.sub(r\"\\t+\", ' ', text)\n",
    "    text = re.sub(r\"\\r+\", ' ', text)\n",
    "    text = re.sub(r\"\\s+\", ' ', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_tashkeel(text):\n",
    "    regx_pattern = re.compile(r'[\\u0617-\\u061A\\u064B-\\u0652]')\n",
    "    text = re.sub(regx_pattern, \"\", text)\n",
    "\n",
    "    regx_pattern = re.compile(r'(.)\\1+')\n",
    "    subst = r\"\\1\\1\"\n",
    "    text = re.sub(regx_pattern, subst, text)\n",
    "    return re.sub(r\"[^\\w\\s]\", '', text)\n",
    "\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return ''.join(c for c in text if c not in punctuation)\n",
    "\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    text_list = []\n",
    "    for w in text.split():\n",
    "        if (not stp.is_stop(w)) and (w not in stop_words):\n",
    "            text_list.append(w)\n",
    "    return \" \".join(text_list)\n",
    "\n",
    "\n",
    "def remove_less_2_characters(text):\n",
    "    return re.sub(r\"\\W*\\b\\w{1,2}\\b\", '', text)\n",
    "\n",
    "\n",
    "def preprocess_text_sample(text):\n",
    "    text = text.lower()\n",
    "    text = remove_emoji(text)\n",
    "    text = remove_email(text)\n",
    "    text = remove_account_tag(text)\n",
    "    text = remove_hashtag(text)\n",
    "    text = remove_links(text)\n",
    "    text = remove_less_2_characters(text)\n",
    "    text = remove_repeated_char(text)\n",
    "    text = remove_punctuation(text)\n",
    "    text = remove_tashkeel(text)\n",
    "    text = remove_stop_words(text)\n",
    "    text = remove_spaces(text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "def preprocess_text_cols(df, col):\n",
    "    df[col] = df[col].apply(lambda x: preprocess_text_sample(x))\n",
    "    return df\n",
    "\n",
    "\n",
    "def preprocess_df(df, col=\"Text\"):\n",
    "    df = preprocess_text_cols(df, col)\n",
    "    df = df[df[col] != \"\"]\n",
    "    df.dropna(inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-12T12:07:26.854938Z",
     "iopub.status.busy": "2022-03-12T12:07:26.854703Z",
     "iopub.status.idle": "2022-03-12T12:07:31.483517Z",
     "shell.execute_reply": "2022-03-12T12:07:31.482016Z",
     "shell.execute_reply.started": "2022-03-12T12:07:26.854905Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "oov_tok = \"<oov_tok>\"\n",
    "\n",
    "\n",
    "def CountVectorizer_fit(X_train, ngram_range=(1,1)):\n",
    "    count_vect = CountVectorizer(ngram_range=ngram_range)\n",
    "    return count_vect.fit(X_train)\n",
    "\n",
    "\n",
    "def CountVectorizer_transform(count_vect, X):\n",
    "    return count_vect.transform(X)\n",
    "\n",
    "\n",
    "def TfidfTransformer_fit(X_train_counts, use_idf=True):\n",
    "    tf_transformer = TfidfTransformer(use_idf=use_idf)\n",
    "    return tf_transformer.fit(X_train_counts)\n",
    "\n",
    "\n",
    "def TfidfTransformer_transform(tf_transformer, X_counts):\n",
    "    return tf_transformer.transform(X_counts)\n",
    "\n",
    "\n",
    "def fit_preprocessing_pipeline(X_train, ngram_range=(1,1), use_idf=True):\n",
    "    pipeline = Pipeline([\n",
    "        (\"vect\", CountVectorizer(ngram_range=ngram_range)),\n",
    "        (\"tfidf\", TfidfTransformer(use_idf=use_idf))])\n",
    "    pipeline.fit(X_train)\n",
    "    return pipeline\n",
    "\n",
    "\n",
    "def transform_preprocessing_pipeline(pipeline, X):\n",
    "    return pipeline.transform(X)\n",
    "\n",
    "\n",
    "def get_max_sequences_len(df, col):\n",
    "    return max([len(x.split()) for x in df[col].values])\n",
    "\n",
    "\n",
    "def get_tokenizer_obj(text_list):\n",
    "    tokenizer = Tokenizer(lower=True, split=\" \", oov_token=oov_tok)\n",
    "    tokenizer.fit_on_texts(text_list)\n",
    "    return tokenizer, len(tokenizer.word_index)\n",
    "\n",
    "\n",
    "def tokenize_texts_to_sequences(tokenizer, text_list):\n",
    "    return tokenizer.texts_to_sequences(text_list)\n",
    "\n",
    "\n",
    "def padding_sequences(x_arr, max_len):\n",
    "    x_arr = pad_sequences(x_arr, maxlen=max_len, value=0, padding='post')\n",
    "    return x_arr\n",
    "\n",
    "\n",
    "def get_max_statment_len(df, col):\n",
    "    return max([len(text.split()) for text in df[col]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-12T12:07:31.485002Z",
     "iopub.status.busy": "2022-03-12T12:07:31.484768Z",
     "iopub.status.idle": "2022-03-12T12:07:31.619291Z",
     "shell.execute_reply": "2022-03-12T12:07:31.618628Z",
     "shell.execute_reply.started": "2022-03-12T12:07:31.484969Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, \\\n",
    "    classification_report, precision_score, recall_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import f1_score as f1_score_rep\n",
    "import keras.backend as K\n",
    "from sklearn.utils import class_weight\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from numpy import unique, newaxis\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def split_dataset(df, y_col=\"\", test_size=0.20, with_stratify=True, shuffle=True):\n",
    "    if with_stratify:\n",
    "        train, val = train_test_split(df,\n",
    "                                      test_size=test_size,\n",
    "                                      random_state=1,\n",
    "                                      stratify=df[y_col],\n",
    "                                      shuffle=shuffle)\n",
    "    else:\n",
    "        train, val = train_test_split(df,\n",
    "                                      test_size=test_size,\n",
    "                                      random_state=1,\n",
    "                                      stratify=df[y_col],\n",
    "                                      shuffle=shuffle)\n",
    "    return train, val\n",
    "\n",
    "\n",
    "def get_label_encoder_obj(y):\n",
    "    label_encoder = LabelEncoder()\n",
    "    return label_encoder.fit(y)\n",
    "\n",
    "\n",
    "def get_y_label_encoder(label_encoder, y):\n",
    "    return label_encoder.transform(y)\n",
    "\n",
    "\n",
    "def get_nb_classes(y):\n",
    "    return len(unique(y))\n",
    "\n",
    "\n",
    "def one_hot_encode(y, num_classes):\n",
    "    return to_categorical(y, num_classes=num_classes)\n",
    "\n",
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    f1_val = 2 * (precision * recall) / (precision + recall + K.epsilon())\n",
    "    return f1_val\n",
    "\n",
    "\n",
    "def get_class_weights(y):\n",
    "    class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                      classes=unique(y),\n",
    "                                                      y=y)\n",
    "    return {k: v for k, v in enumerate(class_weights)}\n",
    "\n",
    "\n",
    "def print_score(y_pred, y_real, label_encoder):\n",
    "    print(\"Accuracy: \", accuracy_score(y_real, y_pred))\n",
    "    print(\"Precision:: \", precision_score(y_real, y_pred, average=\"micro\"))\n",
    "    print(\"Recall:: \", recall_score(y_real, y_pred, average=\"micro\"))\n",
    "    print(\"F1_Score:: \", f1_score_rep(y_real, y_pred, average=\"micro\"))\n",
    "\n",
    "    print()\n",
    "    print(\"Macro precision_recall_fscore_support (macro) average\")\n",
    "    print(precision_recall_fscore_support(y_real, y_pred, average=\"macro\"))\n",
    "\n",
    "    print()\n",
    "    print(\"Macro precision_recall_fscore_support (micro) average\")\n",
    "    print(precision_recall_fscore_support(y_real, y_pred, average=\"micro\"))\n",
    "\n",
    "    print()\n",
    "    print(\"Macro precision_recall_fscore_support (weighted) average\")\n",
    "    print(precision_recall_fscore_support(y_real, y_pred, average=\"weighted\"))\n",
    "\n",
    "    print()\n",
    "    print(\"Confusion Matrix\")\n",
    "    cm = confusion_matrix(y_real, y_pred)\n",
    "    cm = cm.astype('float') / cm.sum(axis=1)[:, newaxis]\n",
    "    df_cm = pd.DataFrame(cm, index=[i for i in label_encoder.classes_],\n",
    "                         columns=[i for i in label_encoder.classes_])\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    sns.heatmap(df_cm, annot=True)\n",
    "\n",
    "    print()\n",
    "    print(\"Classification Report\")\n",
    "    print(classification_report(y_real, y_pred, target_names=label_encoder.classes_))\n",
    "\n",
    "\n",
    "def get_prediction_results(y_true, y_hat, label_encoder, num_classes):\n",
    "    if len(y_true.shape) == 1:\n",
    "        y_train_ohe = one_hot_encode(y_true, num_classes)\n",
    "        y_hat_ohe = one_hot_encode(y_hat, num_classes)\n",
    "    else:\n",
    "        y_train_ohe = y_true.copy()\n",
    "        y_hat_ohe = y_hat.copy()\n",
    "    ROC_plot(y_train_ohe, y_hat_ohe, label_encoder, num_classes)\n",
    "    print_score(y_hat, y_true, label_encoder)\n",
    "\n",
    "\n",
    "def predict(model, X_val):\n",
    "    return model.predict(X_val)\n",
    "\n",
    "\n",
    "def save_model_pkl(model, path_directory, file_name):\n",
    "    joblib.dump(model, os.path.join(path_directory, file_name))\n",
    "\n",
    "\n",
    "def load_model_pkl(file_directory):\n",
    "    return joblib.load(file_directory)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-12T12:07:31.621006Z",
     "iopub.status.busy": "2022-03-12T12:07:31.620766Z",
     "iopub.status.idle": "2022-03-12T12:07:42.957343Z",
     "shell.execute_reply": "2022-03-12T12:07:42.956453Z",
     "shell.execute_reply.started": "2022-03-12T12:07:31.620974Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from livelossplot import PlotLossesKeras\n",
    "from transformers import *\n",
    "from transformers import BertTokenizer, TFBertModel, BertConfig\n",
    "\n",
    "\n",
    "pickle_inp_path = \"Weights\\\\bert_inp.pkl\"\n",
    "pickle_mask_path = \"Weights\\\\bert_mask.pkl\"\n",
    "pickle_label_path = \"Weights\\\\bert_label.pkl\"\n",
    "\n",
    "\n",
    "def tokenizer_decode(bert_tokenizer, tokenized_sequence):\n",
    "    bert_tokenizer.decode(tokenized_sequence['input_ids'])\n",
    "\n",
    "\n",
    "def tokenizer_encode(sentences, labels, max_length):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    for sentence in sentences:\n",
    "        bert_inp = bert_tokenizer.encode_plus(sentence, add_special_tokens=True, max_length=max_length,\n",
    "                                              pad_to_max_length=True, return_attention_mask=True)\n",
    "        input_ids.append(bert_inp['input_ids'])\n",
    "        attention_masks.append(bert_inp['attention_mask'])\n",
    "\n",
    "    input_ids = np.asarray(input_ids)\n",
    "    attention_masks = np.array(attention_masks)\n",
    "    labels = np.array(labels)\n",
    "    return input_ids, attention_masks, labels\n",
    "\n",
    "\n",
    "def save_model_pkl(input_ids, attention_masks, labels):\n",
    "    pickle.dump((input_ids), open(pickle_inp_path, 'wb'))\n",
    "    pickle.dump((attention_masks), open(pickle_mask_path, 'wb'))\n",
    "    pickle.dump((labels), open(pickle_label_path, 'wb'))\n",
    "\n",
    "    print('Pickle files saved as ', pickle_inp_path, pickle_mask_path, pickle_label_path)\n",
    "\n",
    "\n",
    "def load_model_pkl():\n",
    "    print('Loading the saved pickle files..')\n",
    "    input_ids = pickle.load(open(pickle_inp_path, 'rb'))\n",
    "    attention_masks = pickle.load(open(pickle_mask_path, 'rb'))\n",
    "    labels = pickle.load(open(pickle_label_path, 'rb'))\n",
    "    print('Input shape {} Attention mask shape {} Input label shape {}'.format(input_ids.shape, attention_masks.shape,\n",
    "                                                                               labels.shape))\n",
    "\n",
    "\n",
    "def build_bert_model(num_classes):\n",
    "    bert_model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_classes)\n",
    "    print('\\nBert Model', bert_model.summary())\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5, epsilon=1e-08)\n",
    "    bert_model.compile(loss=loss, optimizer=optimizer, metrics=[metric])\n",
    "    return bert_model\n",
    "\n",
    "\n",
    "def fit_bert_model(bert_model, train_inp, train_mask, train_label,\n",
    "                   val_inp, val_mask, val_label, weights_dir):\n",
    "    log_dir = 'tb_bert'\n",
    "    model_save_path = os.path.join(weights_dir, 'bert_model.h5')\n",
    "    callbacks = [ModelCheckpoint(filepath=model_save_path,\n",
    "                                 save_weights_only=True,\n",
    "                                 monitor='val_loss',\n",
    "                                 mode='min',\n",
    "                                 verbose=1,\n",
    "                                 save_best_only=True),\n",
    "                 EarlyStopping(monitor=\"val_loss\", mode=\"min\", verbose=1, patience=5),\n",
    "                 PlotLossesKeras(),\n",
    "                 keras.callbacks.TensorBoard(log_dir=log_dir)]\n",
    "\n",
    "    start_time = time.time()\n",
    "    history = bert_model.fit([train_inp, train_mask], train_label,\n",
    "                             batch_size=32, epochs=20,\n",
    "                             validation_data=([val_inp, val_mask], val_label),\n",
    "                             callbacks=callbacks)\n",
    "    duration = time.time() - start_time\n",
    "    print(\"Model take {} S to train \".format(duration))\n",
    "    return bert_model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-12T12:07:42.960569Z",
     "iopub.status.busy": "2022-03-12T12:07:42.960313Z",
     "iopub.status.idle": "2022-03-12T19:11:57.236221Z",
     "shell.execute_reply": "2022-03-12T19:11:57.235013Z",
     "shell.execute_reply.started": "2022-03-12T12:07:42.960535Z"
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    csv_file_path = \"../input/aimtask/dialect_dataset.csv\"\n",
    "    df = read_csv(csv_file_path)\n",
    "    save_directory_path = \"\"\n",
    "    get_dataset_df(df, save_directory_path)\n",
    "\n",
    "    csv_file_path = \"csv_text_dataset.csv\"\n",
    "    df = read_arabic_csv(csv_file_path)\n",
    "    df = preprocess_df(df, col=\"Text\")\n",
    "\n",
    "    train, val = split_dataset(df, y_col=\"Dialect\", test_size=0.06, with_stratify=True, shuffle=True)\n",
    "    \n",
    "    label_encoder = get_label_encoder_obj(train[\"Dialect\"])\n",
    "    train[\"Dialect\"] = get_y_label_encoder(label_encoder, train[\"Dialect\"])\n",
    "    val[\"Dialect\"] = get_y_label_encoder(label_encoder, val[\"Dialect\"])\n",
    "    \n",
    "    max_statment_len = get_max_statment_len(train, \"Text\")\n",
    "\n",
    "    train_inp, train_mask, train_label = tokenizer_encode(train[\"Text\"], train[\"Dialect\"], max_statment_len)\n",
    "    val_inp, val_mask, val_label = tokenizer_encode(val[\"Text\"], val[\"Dialect\"], max_statment_len)\n",
    "\n",
    "    num_classes = get_nb_classes(train[\"Dialect\"])\n",
    "    bert_model = build_bert_model(num_classes)\n",
    "    weights_dir = \"\"\n",
    "    bert_model, history = fit_bert_model(bert_model, train_inp, train_mask, train_label,\n",
    "                                                  val_inp, val_mask, val_label, weights_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
